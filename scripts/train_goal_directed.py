"""
AION ç›®æ ‡å¯¼å‘å­¦ä¹ è®­ç»ƒè„šæœ¬ (Phase 3) - ä¼˜åŒ–ç‰ˆ

åŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„LSMå’Œä¸–ç•Œæ¨¡å‹
2. å®ç°å¯†é›†å¥–åŠ±æœºåˆ¶ (Dense Rewards - æ”¹è¿›ç‰ˆ)
3. å®ç°è¯¾ç¨‹å­¦ä¹  (Curriculum Learning - å¾ªç¯é‡è¯•)
4. è®­ç»ƒç­–ç•¥/å¾®è°ƒæ¨¡å‹

ä¼˜åŒ–ç‚¹ï¼š
- é™ä½LSMç¥ç»å…ƒæ•°é‡ (1000 -> 400) æå‡CPUé€Ÿåº¦
- å¼•å…¥è§†è§‰å¯¹é½å¥–åŠ± (Visual Centering Reward)
- æ”¹è¿› Level 2 æœç´¢é€»è¾‘ (Systematic Scan)
"""

import sys
import os
import numpy as np
import torch
import random
import time

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.environment_pybullet import PyBulletEnv
from src.lsm import AION_LSM_Network
from src.adapter import RandomProjectionAdapter
from src.hrr import HDCWorldModel
from src.mhn import ModernHopfieldNetwork
from src.config import HDC_DIM

class GoalDirectedTrainer:
    def __init__(self, load_pretrained=True):
        print("=== AION ç›®æ ‡å¯¼å‘å­¦ä¹  (Phase 3) - ä¼˜åŒ–ç‰ˆ ===")
        
        # 1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹
        self.env = PyBulletEnv(headless=True)
        self.lsm = AION_LSM_Network()
        self.adapter = RandomProjectionAdapter()
        self.world_model = HDCWorldModel()
        self.mhn = ModernHopfieldNetwork()
        
        self.action_names = {
            0: "Hover", 1: "Forward", 2: "Rotate_Left",
            3: "Rotate_Right", 4: "Up", 5: "Down"
        }
        self.n_actions = 6
        
        # 2. åŠ è½½é¢„è®­ç»ƒæƒé‡
        if load_pretrained:
            self.load_pretrained_models()
            
    def load_pretrained_models(self):
        """åŠ è½½ Phase 1 & 2 çš„é¢„è®­ç»ƒæƒé‡"""
        print("\næ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        
        # åŠ è½½ LSM è¯»å‡ºå±‚ (å¯é€‰)
        lsm_path = "pretrained_lsm.npz"
        if os.path.exists(lsm_path):
            data = np.load(lsm_path)
            self.W_readout = data['W_readout']
            self.b_readout = data['b_readout']
            print(f"âœ… åŠ è½½ LSM æƒé‡: {lsm_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° LSM æƒé‡: {lsm_path} (å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–)")
            
        # åŠ è½½ World Model & MHN
        wm_path = "pretrained_world_model.pt"
        if os.path.exists(wm_path):
            state_dict = torch.load(wm_path)
            if 'world_model' in state_dict:
                self.world_model.load_state_dict(state_dict['world_model'])
            if 'mhn_memory' in state_dict:
                self.mhn.load_memory(state_dict['mhn_memory'])
            print(f"âœ… åŠ è½½ World Model & MHN: {wm_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° World Model æƒé‡: {wm_path}")
            
    def compute_reward(self, dist, prev_dist, goal_visible, center_x_normalized=0.5):
        """
        å¯†é›†å¥–åŠ±å‡½æ•° (æ”¹è¿›ç‰ˆ)
        """
        reward = 0.0
        
        # 1. è¿›åº¦å¥–åŠ± (Progress)
        delta_dist = prev_dist - dist
        reward += delta_dist * 20.0  
            
        # 2. è§†è§‰å¼•å¯¼å¥–åŠ± (Visual Centering)
        if goal_visible:
            # è¶Šé è¿‘ä¸­å¿ƒï¼Œå¾—åˆ†è¶Šé«˜ (ä¸­å¿ƒåˆ†å¸ƒ 0.5)
            centering = 0.5 - abs(center_x_normalized - 0.5)
            reward += centering * 2.0
            
        # 3. åˆ°è¾¾å¥–åŠ± (Arrival)
        if dist < 0.5:
            reward += 100.0
            
        # 4. ç”Ÿå­˜æƒ©ç½š
        reward -= 0.02
        
        return reward

    def perceive(self, obs):
        """æ„ŸçŸ¥ï¼šå›¾åƒ -> HDCå‘é‡"""
        spikes = self.lsm.step(obs, dopamine=0.0)
        hdc = self.adapter.forward(spikes)
        return hdc, spikes
    
    def detect_goal_simple(self, obs):
        """ç®€å•é¢œè‰²æ£€æµ‹"""
        green_channel = obs[:, :, 1]
        total = obs.shape[0] * obs.shape[1] * 255.0
        ratio = green_channel.sum() / total
        visible = ratio > 0.005
        return visible, ratio

    def train_curriculum(self):
        """è¯¾ç¨‹å­¦ä¹ ä¸»å¾ªç¯ (å¸¦é‡è¯•é€»è¾‘)"""
        levels = [
            {"name": "Level 1: ç›´çº¿è·ç¦» 2m", "dist_range": (1.8, 2.2), "obstacle": False},
            {"name": "Level 2: éšæœºè·ç¦» 1-4m", "dist_range": (1.0, 4.0), "obstacle": False},
            {"name": "Level 3: å«éšœç¢ç‰©", "dist_range": (2.0, 4.0), "obstacle": True},
        ]
        
        level_idx = 0
        while level_idx < len(levels):
            level = levels[level_idx]
            print(f"\n========================================")
            print(f"å¼€å§‹è¯¾ç¨‹ {level['name']}")
            print(f"========================================")
            
            success_count = 0
            total_episodes = 20
            
            for episode in range(total_episodes):
                self.env.reset()
                
                dist = random.uniform(*level['dist_range'])
                angle = random.uniform(0, 2*np.pi)
                goal_pos = np.array([3.0, 0.0, 0.5])
                agent_x = goal_pos[0] - dist * np.cos(angle)
                agent_y = goal_pos[1] - dist * np.sin(angle)
                
                self.env.reset() 
                
                # åˆå§‹æœå‘
                yaw_noise = 0.5 if level_idx == 0 else 3.14
                desired_yaw = np.arctan2(goal_pos[1] - agent_y, goal_pos[0] - agent_x)
                start_yaw = desired_yaw + random.uniform(-yaw_noise, yaw_noise)
                
                self.env.teleport_agent([agent_x, agent_y, 0.5], yaw=start_yaw)
                obs = self.env._get_observation()
                
                prev_dist = dist
                total_reward = 0
                steps = 0
                max_steps = 2000
                done = False
                prev_concept = None
                prev_action = None
                
                print(f"Episode {episode+1}/{total_episodes} | Dist: {dist:.2f}m")
                
                while not done and steps < max_steps:
                    steps += 1
                    current_concept, spikes = self.perceive(obs)
                    goal_visible, _ = self.detect_goal_simple(obs)
                    
                    action = 0
                    green_channel = obs[:, :, 1]
                    green_mask = green_channel > 50
                    center_x_norm = 0.5
                    
                    if green_mask.sum() > 0:
                        cols = np.arange(obs.shape[1])
                        center_x = (green_mask.sum(axis=0) * cols).sum() / green_mask.sum()
                        center_x_norm = center_x / obs.shape[1]
                        
                        if center_x_norm < 0.4:
                            action = 2 # Left
                        elif center_x_norm > 0.6:
                            action = 3 # Right
                        else:
                            action = 1 # Forward
                    else:
                        # æ²¡çœ‹åˆ°ç›®æ ‡ï¼Œç³»ç»ŸåŒ–æ—‹è½¬æœç´¢
                        cycle = steps % 40
                        if cycle < 30:
                            action = 2 
                        else:
                            action = 1
                            
                    next_obs, env_reward, terminated, truncated, info = self.env.step(action)
                    curr_pos = np.array(self.env.get_pos())
                    curr_dist = np.linalg.norm(curr_pos - goal_pos)
                    
                    dense_reward = self.compute_reward(curr_dist, prev_dist, goal_visible, center_x_norm)
                    total_reward += dense_reward
                    
                    if prev_concept is not None:
                        self.world_model.learn(prev_concept, prev_action, current_concept)
                    
                    prev_dist = curr_dist
                    prev_concept = current_concept
                    prev_action = action
                    obs = next_obs
                    
                    if curr_dist < 0.5:
                        print(f"  âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡! Reward: {total_reward:.2f}")
                        success_count += 1
                        done = True
                        
                if not done:
                    print(f"  âŒ è¶…æ—¶/å¤±è´¥. Reward: {total_reward:.2f}")
                    
            success_rate = success_count / total_episodes
            print(f"è¯¾ç¨‹ {level['name']} å®Œæˆç‡: {success_rate:.2%}")
            
            if success_rate < 0.7:
                print("âš ï¸ è¡¨ç°ä¸ä½³ï¼Œé‡è¯•å½“å‰éš¾åº¦...")
                continue 
            else:
                print("ğŸ‰ æ™‹çº§ä¸‹ä¸€éš¾åº¦ï¼")
                level_idx += 1
                
            # Save models after each level
            self.save_models()
            
    def save_models(self):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        print(f"æ­£åœ¨ä¿å­˜æ¨¡å‹æƒé‡ ({timestamp})...")
        
        # 1. Save World Model & MHN
        torch.save({
            'world_model': self.world_model.M_per_action, # Save the list of tensors directly? Or state dict
            # HDCWorldModel structure changed slightly in our fix. 
            # We should probably add a proper state_dict method to HDCWorldModel if not exists, 
            # or just save the list.
            # Let's save the list for simplicity as we implemented load_state_dict to take list.
            'mhn_memory': self.mhn.memory_matrix
        }, "pretrained_world_model.pt")
        
        # 2. Save LSM Readout (If we had one training)
        # In this script we don't train readout explicitly (LSM is fixed reservoir).
        # But if we did, we'd save it.
        print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° pretrained_world_model.pt")

def main():
    trainer = GoalDirectedTrainer(load_pretrained=True)
    trainer.train_curriculum()

if __name__ == "__main__":
    main()
